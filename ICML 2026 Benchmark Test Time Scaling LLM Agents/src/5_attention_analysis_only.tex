\section{Attention Behavior Analysis}\label{section5}

\begin{figure*}[h]
    \centering

    \begin{subfigure}{0.9\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/attention_analysis_final_qwen3_235B.pdf}
        % \caption{Qwen3-235B}
        \label{fig:attn_qwen3_235b}
    \end{subfigure}

    \vspace{6pt}

    \begin{subfigure}{0.9\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/attention_analysis_final_qwen3_next.pdf}
        % \caption{Qwen3-Next}
        \label{fig:attn_qwen3_next}
    \end{subfigure}

    \caption{Comparison of attention behaviors under the General AgentBench setting. Top: Qwen3-235B (full attention). Bottom: Qwen3-Next (hybrid linear attention).}
    \label{fig:attention_analysis}
\end{figure*}


In sequential test-time scaling, we observe that Qwen3-Next demonstrates weaker scaling potential compared to other models, particularly its full-attention counterpart, Qwen3-235B-A22B. Although these models differ in training data composition and parameterization, we analyze this discrepancy primarily from the perspective of the attention mechanism—the most salient architectural difference between them. We first introduce our analysis methodology, followed by empirical results and key findings.
 

\subsection{Extracting Top-K Attention Tokens for Reasoning Behavior}

Our attention analysis is conducted on inference trajectories collected from General AgentBench. For each benchmark domain, we randomly sample 25 trajectories generated by the model under analysis. Because attention patterns depend strongly on the specific text region being examined, we follow the framework of \citet{jin2025beneficial} to extract reasoning-behavior sentences from each trajectory. These sentences correspond to critical decision-making steps, where the model actively reasons over accumulated context. For each reasoning sentence, we iterate over all tokens within the sentence. For every token, we compute and store its attention distribution over the preceding context. We then average the attention scores across tokens in the reasoning sentence, select the top-$K$ attended tokens ($K = 128$), and record their token indices. This procedure is applied to all layers and attention heads.

We evaluate attention behavior using two metrics:

\paragraph{Mean Attention Distance.} For each of the top-$K$ attended tokens, we compute its token distance from the reasoning sentence and weight it by the corresponding attention score. This metric measures how far back the model attends when making a key decision, effectively quantifying the model’s effective contextual view.

\paragraph{Top-K Overlap.}
We measure the overlap of top-$K$ attended tokens through intra-layer overlap (across heads within the same layer) and inter-layer overlap ( across different layers). Higher overlap indicates that different heads or layers attend to similar contextual tokens, suggesting reduced functional differentiation.

For Gated DeltaNet, which does not explicitly expose standard attention weights, we adopt a mathematically equivalent reconstruction procedure to recover the effective top-$K$ contributing tokens and compute the corresponding distance statistics.

\subsection{Results and findings}

In the leftmost panel of Figure~\ref{fig:attention_analysis}, we visualize the mean attention distance for each head across layers. Full-attention models exhibit consistently larger mean distances, with only minor exceptions in early layers and around layer 70. In most layers, the majority of heads attend to long-range context, while a small subset focuses on local patterns. This behavior is also observed in the gated full-attention layers of Qwen3-Next (appearing every four layers). These results suggest that full attention maintains a broader effective contextual view than linear attention, consistent with the convolution-like receptive field constraints imposed by DeltaNet-style linear attention \cite{yang2024gated}.

The two rightmost panels present the top-$K$ token overlap statistics.

\paragraph{Intra-layer overlap.}
Full attention exhibits a characteristic V-shaped curve: middle-layer heads attend to more diverse patterns, while later layers converge toward similar tokens, reflecting increased certainty near the final decision stage. In contrast, linear attention lacks a clear structural pattern and shows higher average intra-layer overlap, suggesting reduced head specialization.

\paragraph{Inter-layer overlap.}
Full attention displays a gradual ``low-to-high'' trend across depth, indicating that adjacent layers share similar functional roles, while functional divergence accumulates progressively with depth. In linear-attention models, DeltaNet layers show very low overlap with gated full-attention layers. Although DeltaNet layers exhibit high overlap across distant layers (indicating homogeneous behavior), the full-attention layers preserve their characteristic inter-layer structure.

Overall, our findings indicate that linear attention demonstrates weaker functional differentiation across heads and layers, along with reduced long-context utilization in agentic reasoning tasks, compared to full-attention mechanisms.