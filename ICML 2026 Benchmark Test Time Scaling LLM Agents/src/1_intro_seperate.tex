\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/intro_figure_radar_1x3.pdf}
    \caption{\textbf{Evaluating agents under a realistic user-interaction scenario.} \textbf{Left}: When evaluated within Omni AgenticBench framework, GPT-5 exhibits a substantial performance drop compared to static evaluations with fixed, pre-specified contexts. \textbf{Right above}: Sequential test-time scaling, allocating additional computation through extended interaction histories, often leads to unstable performance and can even degrade agent accuracy, despite increased compute. \textbf{Right bottom}: Although correct solutions increasingly appear in the model’s generation space as the number of samples grows (blue, best@k), agents frequently fail to identify these solutions (red line), revealing a pronounced verification gap between generation and self-cognition.}
    \label{fig:intro_figure}
\end{figure*}

Developing general-purpose agents powered by Large Language Models (LLMs) has emerged as a central focus of contemporary AI research. These agents, equipped with real-world tools and strong reasoning capabilities inherited from LLMs, are deployed to interact with human users and solve a wide variety of complex, open-ended requests. To evaluate such capabilities, numerous agentic benchmarks have been proposed, covering major task categories including coding, web search, tool calling, and computer-use. These benchmark designers typically curate distinct, difficulty-varied questions to comprehensively test agent performance within specific, isolated environments.

However, the complexity of real-world user queries stems not only from content diversity, but also from the absence of explicit task-category assumptions during user–agent interaction. Existing agentic benchmarks typically evaluate agents under predefined, domain-specific contexts with restricted toolsets, which are well suited for assessing specialized agents within known task settings. For example, SWE-bench exposes only Bash-related tools to facilitate software engineering tasks, while BrowserComp defines a single search tool to support web navigation. In contrast, general agents operate without prior knowledge of a query’s domain and must first infer the underlying task type before selecting appropriate tools. As a result, existing evaluation settings do not explicitly model this form of task-category uncertainty, leaving the general-agent setting underexplored in current benchmarks.\cx{this paragraph can use more revision, high level real world need argument, and also focus more on this work, less than swebench etc, also argue that not tast uncertainty, but composition of different skills and tools in on complicated task that span across search, coding, and math, etc.}

In this paper, we bridge this gap by introducing Omni AgenticBench, a benchmark for evaluating general-purpose agents across diverse scenarios—including search, coding, reasoning, and tool use—under a unified toolset framework that more closely reflects real-world interactions with users. We began by auditing widely used benchmarks to ensure our task selection remains high-quality and representative of state-of-the-art challenges. \cx{missing a general overview of our benchmark, use the previous sentence's spacae for that?} We consolidated tool definitions and descriptions from all domains into a single, unified toolsets. By replacing domain-specific policy prompts with a global toolset and a general policy, we evaluate agents in a setting that more closely mirrors real-world interactions, where task domains are not known in advance. Our evaluation of ten leading LLMs under this setting shows a significant performance degradation when moving from domain-aware configurations to a general-agent context. \cx{here can discuss different properties of differenet llms}


A defining aspect of agentic capability lies in maintaining and reasoning over long, evolving contexts. First, the unified toolset consists of numerous tool descriptions, parameters, and general policy instructions, which together form a substantial long-context input. Second, multi-turn interactions between agents, users, and environments continuously expand the context through accumulated trajectories.  Rather than treating long-context capability as a static property that is commonly evaluated by previous benchmarks such as Needle In A Haystack or long-document QA, we study how agents behave when operating under long and dynamically growing contexts. Specifically, we use \textbf{test-time scaling} as a practical way to examine whether agents can reason consistently, make stable decisions, and effectively use information as the context length increases. We conduct a systematic study of two primary scaling methods: (1) parallel scaling, which independently samples $K$ trajectories; and. (2) sequential scaling, which introduces additional interaction turns to encourage iterative reflection. Our results across five models demonstrate that while parallel scaling yields expected improvement, sequential scaling exhibits the opposite trend: nearly all models suffer performance degradation despite increased computation. In some cases, performance falls below the single-pass baseline, suggesting that even frontier models struggle to reason effectively over raw, accumulated trajectory contexts. We also demonstrate that high performance on static long-context tasks does not necessarily transfer to the dynamic, multi-turn requirements of agentic tasks.
% As a result, agents must reason over extended histories that include tool specifications, prior decisions, and intermediate results.

While multiple sampling often achieve high accuracy, this merely indicates that a correct solution exists somewhere within the sampled solution space. It does not guarantee that the agent can identify that solution. In real-world interactions, an agent cannot present $K$ responses simultaneously and must instead commit to a single answer. Consequently, for parallel scaling to be practically effective, an agent must not only generate correct candidates but also recognize the best one. This capability is critical for autonomous agents, particularly in non-verifiable settings. To evaluate this ability, we move beyond best@$K$ and introduce self-selection, where the agent must assess its own candidates and select a final answer. Our results reveal a substantial gap between best@$K$ performance and self-selection accuracy, highlighting a fundamental limitation in agent self- verification. \cx{you are presenting the work sequentially as separate parts in the order of them appear in the paper, this is wrong. Really present as the introduction structure: what we do and how we do it, as a general benchmark that covers many important aspects of general agents, then discuss the overal findings (all together), insights (all together), future potentials (all together)}

To explain the performance degradation observed under the general-agent setting, we conduct a targeted analysis of agent failure modes. First, we analyze agent behavior from the perspective of cross-domain tool selection. Under the unified toolset, agents must infer both task type and appropriate tool usage. By comparing trajectories under the original benchmark settings and the general-agent setting, we identify three representative patterns: (i) agents fail after invoking tools from incorrect domains; (ii) agents initially explore incorrect-domain tools but recover by switching to the correct tool and succeed; and (iii) agents select the correct tool throughout yet still fail under the general setting. Notably, performance on certain search benchmarks improves under the unified toolset, indicating that benchmark-specific tool restrictions may limit the observable capability of current agents. Second, we analyze outcome transitions under sequential test-time scaling by tracking how predictions evolve with additional computation. We measure how often additional interaction steps turn an initially incorrect solution into a correct one, and how often they instead cause a previously correct solution to fail, helping explain why sequential scaling frequently underperforms despite increased computation.

% Together, these analyses provide concrete explanations for the observed failures in realistic, long-horizon general-agent settings.

% Finally, the long-context nature of agentic tasks has motivated the development of alternative attention mechanisms to mitigate the $\mathcal{O}(n^2)$ complexity of standard scaled dot-product attention. In our evaluation, we include production models such as Qwen3-Next and DeepSeek-v3.2 to study the effects of sparse and linear attention mechanisms in agentic settings. Beyond aggregate performance, we analyze attention patterns along agent trajectories to examine the relationship between historical context and reasoning behavior. By extracting top-$k$ attention tokens associated with key reasoning steps, we characterize attention distance and token overlap patterns unique to agentic workflows. While certain similarities with non-agentic tasks per   sist, we observe notable differences driven by long-horizon interaction and tool usage. We further present qualitative case studies that map top-$k$ attention tokens back to the input context, providing interpretable insights into how specific historical information triggers agent reasoning.

In summary, our contributions are:

% \begin{itemize}
%     \item \textbf{Omni AgenticBench:} We introduce Omni AgenticBench, a novel evaluation framework designed to eliminate the prevalent issue of \textbf{domain leakage}. By decoupling domain-specific hints from task execution, it provides a rigorous and unbiased assessment of an agent’s ability to reason over long-horizon contexts across diverse domains.
%     \item \textbf{Characterization of Test-Time Scaling:} We present a systematic study of test-time scaling behaviors in agentic settings. Our analysis identifies a critical bottleneck in model self-cognition: while parallel scaling expands the solution space, models frequently fail to identify the correct answer among their samples. We also reveal that sequential scaling often leads to performance degradation in complex agentic workflows.
%     \item \textbf{Mechanistic Analysis across Attention Architectures:} We utilize Omni AgenticBench as a diagnostic testbed to uncover distinct attention patterns across various architectures, including linear and sparse attention mechanisms. Through our reasoning-behavior framework, we quantify the functional diversity and effective attention distance of different layers and heads, mapping specific contextual triggers to reasoning behaviors.
% \end{itemize}

\begin{itemize}
    \item \textbf{Omni AgenticBench for Real-World Agent Evaluation.} 
    We introduce Omni AgenticBench, a benchmark designed to narrow the gap between existing agent evaluations and real-world user interactions. By providing unified toolsets and instructions, the benchmark evaluates whether agents can infer task intent, select appropriate tools, and reason over long, evolving interaction contexts in open-ended settings.

    \item \textbf{Systematic Test-Time Scaling Behaviors of General Agents} 
    We analyze how agents reason over long, evolving contexts using test-time scaling as a diagnostic tool. By comparing parallel and sequential scaling, we show that sequential scaling often degrades performance as interaction history grows, exposing limitations in agents’ ability to maintain stable reasoning over accumulated contexts. We further identify a self-cognition bottleneck: although parallel scaling expands the solution space, agents frequently fail to recognize correct solutions among their own samples, limiting their effectiveness in real-world interactions.

    \item \textbf{Failure Analysis of General-Agent Behavior.} 
    We analyze agent failure modes under unified toolsets, focusing on cross-domain tool selection errors and performance instability under sequential test-time scaling. Our results show that agents are often misled by irrelevant tools and that longer interaction histories more frequently degrade correct solutions than fix incorrect ones, explaining the observed performance drop in general-agent settings.

\end{itemize}


\cx{intro has the right element and the story reads coherent now. need revision on reorg the content to fit in the right structure of introduction, polish the language to be more concise (now it is too long too), and twist some argument and statement to be coherent (and accurate)}

\cx{we need related work section. perhaps can be the last section before conclusion. need to cover single agentic benchmarks, general agent development, and test time scaling work}

% Furthermore, as our merged "Omni-toolset" exceeds 64K tokens, the benchmark inherently serves as a rigorous test of long-context utilization. By analyzing the correlation between Omni AgenticBench and traditional long-context benchmarks such as Needle In A Haystack and long-document QA, we demonstrate that high performance on static long-context tasks does not necessarily transfer to the dynamic, multi-turn requirements of agentic tasks.


% We examine two selection methodologies: point-wise, where the agent evaluates generations individually, and pair-wise, where the agent compares two generations at a time using an iterative approach similar to bubble sort. 