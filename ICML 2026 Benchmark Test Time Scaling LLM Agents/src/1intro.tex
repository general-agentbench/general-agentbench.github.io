\begin{abstract}

True agentic ability requires solving long-horizon agentic tasks without reliance on domain-specific hints. However, existing benchmarks often suffer from domain leakage, where provided tool descriptions inadvertently introduce prior knowledge bias. To address this, we present Omni AgenticBench, a benchmark spanning four domains that explicitly removes such leakage to enable fair evaluation. Additionaly, we systematically investigate test-time scaling strategies, revealing distinct and often counterintuitive performance trends across models. Beyond standard pass@k metrics, we introduce a self-choice evaluation, where models identify the correct final answer from multiple generated candidates. Our results expose a substantial gap between the models' solution space and their self-cognition space. Finally, we analyze attention mechanisms of recent linear and sparse architectures under agentic settings. All tasks, code, and data are publicly available.

\end{abstract}


\section{Introduction}

% \cx{make this a benchmark paper? then the studies are insights we have with the benchmark. title is not reflecting this.}

Developing general-purpose agents powered by Large Language Models (LLMs) has emerged as a primary focus of current AI research\cite{yao2022react, liu2023agentbench, schick2023toolformer}. A defining characteristic of agentic tasks is their reliance on extensive context; predefined tool descriptions, complex user queries, model-generated reasoning, and environmental feedback collectively form intricate, long-context, multi-turn interaction histories\cite{qin2023toolllm,zhou2023webarena,yang2024sweagent}. Consequently, the performance of these agents is inextricably linked to the model’s ability to reason over both tool specifications and long-horizon information.

While agentic benchmarks are well-established for evaluating frontier models, their evaluation protocols often suffer from a critical shortcoming. To achieve true generality, future agents must solve problems without prior assumptions regarding the task content\cite{liang2025sweillusion} or category. However, existing benchmarks typically provide only domain-specific tool descriptions for a given task, resulting in a constrained and narrow evaluation environment. For instance, SWE-bench\cite{jimenez2023swebench} provides only bash commands tools in their policy prompts to facilitate software engineering tasks, while BrowserComp\cite{wei2025browsecomp} defines only a search tool to assist with web navigation without other tools' interference. We argue that these predefined toolsets unintentionally signal the task's domain to the agent—a phenomenon we term \textbf{``domain leakage''}—thereby simplifying the problem and failing to test the agent's ability to operate in a truly open-ended environment.

In this paper, we bridge this gap by introducing Omni AgenticBench, a benchmark designed to comprehensively evaluate agentic capabilities across diverse categories—including search, coding, reasoning, and tool-use—within a unified toolset framework. We began by auditing widely used agent benchmarks and evaluating frontier open-source models to ensure our task selection was both high-quality and representative of state-of-the-art challenges. To eliminate domain leakage, we consolidated tool definitions and descriptions from all domains into a single, unified library. We then replaced domain-specific policy prompts and constrained toolsets with this global library and a set of general instructions. This configuration allows us to decouple an agent's true reasoning ability from the performance gains provided by domain-specific hints. Our comprehensive evaluation of ten leading LLMs reveals a significant performance degradation for most agents when prior domain hints are removed. Interestingly, a subset of models maintains or even marginally improves performance, suggesting a higher degree of robust, generalizable agency. Furthermore, because our merged "Omni-toolset" accumulates to approximately 64K tokens, the benchmark inherently serves as a rigorous test of long-context utilization. By analyzing the correlation between Omni AgenticBench and traditional long-context benchmarks—such as Needle In A Haystack\cite{kamradt_needlehaystack_2023} and long-document QA\cite{bai2025longbenchv2}—we demonstrate that high performance on static long-context tasks does not necessarily transfer to the dynamic requirements of agentic workflows.

% We evaluate not only frontier large language models (LLMs) on Agentic LongBench to ground their long-context abilities in agentic scenarios, but also include recent innovations in attention design—such as Qwen3-Next and DeepSeek-v3.2—to reveal how modifications to the attention mechanism affect performance in stressful real-world cases. Although the adoption of sparse attention and linear-attention mixture architectures aims for higher efficiency and reduced memory bottlenecks, their performance still lags significantly behind established models like GPT-5 and Claude 4.5 Sonnet, highlighting a persistent performance gap for these attention variants. Furthermore, we evaluate these models on existing representative long-context benchmarks and compare their score correlations with our findings. The low correlation between the two suggests that single-turn long-context proficiency does not directly transfer to agentic settings. To provide a more robust assessment, we introduce the Consistency-Accuracy Index (CAI), which balances raw performance with stability. Our findings show that, with the notable exception of GPT-5, most models exhibit unstable ranking trends, struggling to maintain high mean accuracy alongside behavioral consistency.

% While the context window of backbone LLMs has scaled to millions of tokens, effectively eliciting and evaluating their long-context potential remains an open challenge. Existing benchmarks designed to assess long-context abilities suffer from two critical limitations: (1) Task Distribution Mismatch: Traditional benchmarks often rely on long-document Question Answering (QA) or summarization, simply scaling the input size without incorporating the core elements of agentic workflows—namely, dynamic environments, tool-use, and nuanced user intents. (2) Structural Mismatch: Current benchmarks are predominantly single-turn. This format fails to capture the complexity of agentic tasks, where the long context is composed of the model’s own historical actions, such as command executions, intermediate summaries, and self-reflections. These recursive dependencies are fundamentally absent in single-turn evaluation datasets, therefore failed to mimic real world agent tasks. Furthermore, current agent related benchmarks often suffer from scope constraints: they either lack tool integration, which artificially restricts the model's action space, or focus exclusively on niche domains such as machine learning engineering or gaming. Consequently, they fail to provide a comprehensive analysis of the relationship between task complexity and context length.

% In this paper, we address these gaps by introducing Agentic LongBench, a benchmark specifically curated to comprehensively evaluate agentic capabilities across diverse task categories, including search, coding, reasoning, and tool-use domains. We evaluate not only frontier large language models (LLMs) on Agentic LongBench to ground their long-context abilities in agentic scenarios, but also include recent innovations in attention design—such as Qwen3-Next and DeepSeek-v3.2—to reveal how modifications to the attention mechanism affect performance in stressful real-world cases. Although the adoption of sparse attention and linear-attention mixture architectures aims for higher efficiency and reduced memory bottlenecks, their performance still lags significantly behind established models like GPT-5 and Claude 4.5 Sonnet, highlighting a persistent performance gap for these attention variants. Furthermore, we evaluate these models on existing representative long-context benchmarks and compare their score correlations with our findings. The low correlation between the two suggests that single-turn long-context proficiency does not directly transfer to agentic settings. To provide a more robust assessment, we introduce the Consistency-Accuracy Index (CAI), which balances raw performance with stability. Our findings show that, with the notable exception of GPT-5, most models exhibit unstable ranking trends, struggling to maintain high mean accuracy alongside behavioral consistency.

The multi-turn interaction format of agentic tasks make it easy for agents to allocate more compuation and enable test-time scaling: by forcing more turns and encouraging agents to reflect, we expect models to utilize their reasoning abilities to revise answers, retrieve missing information from the context, or develop new reasoning paths. While test-time scaling trend has been well studied for pure reasoning tasks, the behavior of agent scaling remains under-explored. We provide a systematic study of two primary scaling methods in agentic settings: (1) parallel Scaling: independently sampling $K$ trajectories and calculating accuracy via the best@$K$ metric. (2) sequential scaling: monitoring the model's "closing intent" and manually intervening with an additional turn of encouragement to prompt further reflection and thought. Our results across five models demonstrate that while parallel scaling follows expected improvement trends, sequential scaling exhibits the opposite phenomenon: nearly all models suffer performance degradation even with four times the computation. In some cases, sequential scaling performance falls below the baseline evaluation. Further analysis reveals that current LLMs either struggle to identify their own errors buried within the context or fail to maintain their initial correct answers, eventually overturning them. This indicates that even frontier models struggle to reason effectively over raw, accumulated trajectory contexts.

While agents often achieve high best@$K$ scores, this metric merely indicates that a human can verify the task and that a correct solution exists within the agent’s solution space. It provides no guarantee that the agent can identify that correct generation. For parallel scaling to be effective, an agent must not only sample the correct answer but also "recognize" it. This capability is essential for agents to improve autonomously, particularly in non-verifiable tasks. To address this, we move beyond simple best@$K$ metrics to evaluate "self-selection"—a setting where the agent must evaluate its own candidates and select a final answer. We examine two selection methodologies: point-wise, where the agent evaluates generations individually, and pair-wise, where the agent compares two generations at a time using an iterative approach similar to bubble sort. Under this framework, a task is successful only if a correct generation is both produced and selected by the model. Our results reveal a significant gap between best@$K$ results and self-selection accuracy, highlighting a critical need to improve agent self-cognition to better align a model's judgment with its internal solution space.

% Consequently, agentic tasks inherently integrate long-context processing with reasoning. Their features to easily allocate additional computation makes them an ideal testbed for analyzing test-time scaling behavior. While test-time scaling has proven effective for pure reasoning tasks, its application to agentic workflows remains under-explored. We provide a systematic study of two primary scaling methods in agentic settings: (1) Parallel Scaling: Independently sampling $K$ trajectories and calculating accuracy via the Best@$K$ metric. (2) Sequential Scaling: Monitoring the model's "closing intent" and manually intervening with an additional turn of encouragement to prompt further reflection and thought. Our results across five models demonstrate that while parallel scaling follows expected improvement trends, sequential scaling exhibits the opposite phenomenon: nearly all models suffer performance degradation even with four times the computation. In some cases, sequential scaling performance falls below the baseline evaluation. Further analysis reveals that current LLMs either struggle to identify their own errors buried within the context or fail to maintain their initial correct answers, eventually overturning them. This indicates that even frontier models struggle to reason effectively over raw, accumulated trajectory contexts.

Finally, we demonstrate that Omni AgenticBench serves as a powerful diagnostic testbed through an attention analysis. Our study covers standard scaled dot-product attention as well as architectural innovations such as sparse and linear attention. We utilize a reasoning-behavior framework to identify critical reasoning spans within the input text, subsequently computing accumulated attention over these spans to extract the top-$k$ most influential tokens. By calculating the top-$k$ token overlap across layers (inter-layer) and heads (intra-layer), we quantify the functional diversity and redundancy inherent in different architectures. We further characterize the "effective receptive field" of these components by computing the average attention distance.  Finally, we provide case studies by mapping top-$k$ attention tokens back to the input text, revealing the specific contextual information that triggers reasoning behaviors. In summary, our contributions are:

% Finally, we demonstrate that Omni AgenticBench can serve as a powerful diagnostic testbed by providing an additional attention patterns analysis. We adopt a reasoning-behavior framework to identify key reasoning spans in the text first. Then, by computing accumulated attention over these spans and mapping top-$k$ scores back to tokens, we investigate which contextual information triggers reasoning. While some high-attention tokens correspond to relevant content (e.g., location tokens triggering geographical query modifications), many high-scoring tokens lack semantic meaning, reflecting classic issues such as attention sinks and local attention bias. In our context engineering analysis, we contrast task-specific tool availability with an "Omni-setting" (providing all available tools regardless of the task). This removes the prior bias of pre-filtered toolsets. Our results show a nearly universal performance drop in the Omni-setting, challenging the community to build agents that are truly environment-agnostic. In summary, our contributions are:



\begin{itemize}
    \item \textbf{Omni AgenticBench Framework:} We introduce Omni AgenticBench, a novel evaluation framework designed to eliminate the prevalent issue of \textbf{domain leakage}. By decoupling domain-specific hints from task execution, it provides a rigorous and unbiased assessment of an agent’s ability to reason over long-horizon contexts across diverse domains.
    \item \textbf{Characterization of Test-Time Scaling:} We present a systematic study of test-time scaling behaviors in agentic settings. Our analysis identifies a critical bottleneck in model self-cognition: while parallel scaling expands the solution space, models frequently fail to identify the correct answer among their samples. We also reveal that sequential scaling often leads to performance degradation in complex agentic workflows.
    \item \textbf{Mechanistic Analysis across Attention Architectures:} We utilize Omni AgenticBench as a diagnostic testbed to uncover distinct attention patterns across various architectures, including linear and sparse attention mechanisms. Through our reasoning-behavior framework, we quantify the functional diversity and "effective receptive fields" of different layers and heads, mapping specific contextual triggers to reasoning behaviors.
\end{itemize}

% First, mechanistic analysis reveals a weak correlation between high-attention tokens and semantic reasoning, often overshadowed by attention sinks and local bias. Second, by introducing the "Omni-setting," we demonstrate a universal performance drop when agents are not provided with pre-filtered toolsets, highlighting the need for more environment-agnostic agent architectures.