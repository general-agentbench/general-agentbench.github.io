\section{Related Work}

Prior work on LLM agents spans three closely related directions: benchmarking agentic abilities, developing agent models and frameworks, and exploring effective scaling methods. Most agentic benchmarks evaluate specialized agents in domain-specific settings, where tasks, interaction protocols, and tool access are tailored to specific skills. Representative ones cover software engineering (e.g., SWE-bench \cite{jimenez2023swebench} and its variants \cite{Aleithan2024SWEBenchPlus,Zhang2025SWEBenchGoesLive,yang2024sweagent}), web navigation and interaction (e.g., WebShopcite \cite{Yao2022WebShop}, Mind2Web \cite{Deng2023Mind2Web}, WebArena \cite{zhou2023webarena}, WebVoyager \cite{he2024webvoyager}), and tool use with curated APIs (e.g., ToolLLM \cite{Qin2023ToolLLM}, API-Bank \cite{Li2023APIBank}, BFCL \cite{patil2025bfcl}, StableToolBench \cite{Guo2024StableToolBench}). Broader suites such as $\tau$-bench \cite{Yao2024TauBench}, AgentBench\cite{barres2025tau2}, and GAIA \cite{Mialon2023GAIA} incorporate multi-turn interaction across multiple task categories , while OSWorld \cite{xie2024osworld}, OSWorld-G \cite{xie2025jedi}, and AndroidWorld \cite{rawles2024androidworld} evaluate agents operating in real desktop and mobile systems.

In parallel, substantial effort has focused on building general-purpose agents capable of planning, acting, and invoking tools across heterogeneous tasks. Early methods such as ReAct \cite{yao2022react}, Reflexion \cite{shinn2023reflexion}, Toolformer \cite{schick2023toolformer}, and HuggingGPT \cite{Shen2023HuggingGPT} introduce structured reasoning–action trajectories, reflection, and learned tool invocation. More recent work emphasizes scalable agent frameworks and deployment platforms supporting multi-agent coordination and rich tool ecosystems, including OpenAgents \cite{Xie2023OpenAgents}, AgentVerse \cite{chen2023agentverse}, AgentScope \cite{gao2024agentscope}, and OpenCUA \cite{wang2025opencua}. Industry systems—such as agents built on Claude \cite{claudeagent}, Microsoft \cite{microsoftagent}, OpenAI \cite{openaiagent}, Qwen \cite{Qwenagent}, Kimi \cite{moonshotai2026kimi-k25}, and Gemini \cite{gemini}—further demonstrate the practical importance of general agents in real-world deployments.

A complementary line of research studies test-time scaling, allocating additional inference-time computation to improve agent performance. Chain-of-thought prompting \cite{Wei2022CoT} and self-consistency \cite{Wang2022SelfConsistency} show that deeper reasoning and multi-sample decoding can substantially boost accuracy. More explicit approaches perform search or refinement over solution paths, including Tree-of-Thoughts \cite{Yao2023ToT}, MCTS-based agent planning such as LATS \cite{Zhou2023LATS}, and iterative refinement methods like Reflexion \cite{shinn2023reflexion} and Self-Refine \cite{Madaan2023SelfRefine}. More recently, ``internal scaling’’ trains models to autonomously decide how much inference computation to allocate and when to terminate reasoning, shifting control from external orchestration toward model-internal deliberation \cite{guo2025deepseek, openai2024learning-to-reason}. Verifier-based inference further augments sampling or search with learned ranking or rejection, with process-level supervision improving reliability in mathematical reasoning \cite{ Lightman2023VerifyStepByStep, li2024montessori, hosseini2024v, wang2024math}. 