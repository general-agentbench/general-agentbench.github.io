\section{Conclusions}
We present General AgentBench, a unified benchmark for evaluating LLM agents under realistic, multi-domain interactions where agents must infer intent, select tools from a shared pool, and act end-to-end. Across ten leading models, we find a substantial robustness gap when moving from domain-specific to general-agent evaluation. Our test-time scaling analysis reveals two fundamental limits: sequential scaling is bounded by an context ceiling, beyond which longer interactions become unstable, and parallel scaling delivers limited practical gains due to a persistent verification gap between generation and self-choice. We hope General AgentBench enables realistic assessment and guides progress toward robust, scalable general agents.
